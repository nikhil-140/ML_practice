{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvO8/bP9fH3fphVLrnxv9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhil-140/ML_practice/blob/main/ML_Concepts_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean and Explore Employee Dataset"
      ],
      "metadata": {
        "id": "35pMjB09HHAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Load Dataset\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"employee_practice.csv\")  # <-- Fill: load CSV file\n",
        "print(\"First 5 rows:\\n\", df.head())\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Identify Features & Label and Print features columns and label name\n",
        "# -----------------------------\n",
        "features = df.drop(\"Promotion_Eligibility\", axis=1)\n",
        "label = df[\"Promotion_Eligibility\"]\n",
        "\n",
        "print(\"\\nFeatures:\\n\", features)  # <-- Fill: print feature columns\n",
        "print(\"Label:\\n\", label)   # <-- Fill: print label name\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Handle Missing Values\n",
        "# -----------------------------\n",
        "print(\"\\nMissing Values Before:\\n\", df.isnull().sum())  # <-- Fill: check missing values\n",
        "\n",
        "# Fill numerical missing values with median\n",
        "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())  # <-- Fill: handle missing Age\n",
        "df[\"Years_Experience\"] = df[\"Years_Experience\"].fillna(df[\"Years_Experience\"].median())  # <-- Fill: handle missing Years_Experience\n",
        "\n",
        "# Fill categorical missing values with mode\n",
        "df[\"Department\"] = df[\"Department\"].fillna(df[\"Department\"].mode([1]))  # <-- Fill: handle missing Department\n",
        "df[\"Promotion_Eligibility\"] = df[\"Promotion_Eligibility\"].fillna(df[\"Promotion_Eligibility\"].mode([1]))  # <-- Fill: handle missing Promotion_Eligibility\n",
        "\n",
        "print(\"\\nMissing Values After:\\n\", df.isnull().sum())\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Detect & Handle Outliers (Simple Method)\n",
        "# -----------------------------\n",
        "# Identify Salary outliers (e.g., Salary < 30000 or Salary > 150000)\n",
        "salary_outliers = df[(df[\"Salary\"] < 30000) | (df[\"Salary\"] > 150000)]  # <-- Fill: set low and high threshold values\n",
        "print(\"\\nSalary Outliers:\\n\", salary_outliers)\n",
        "\n",
        "# Remove Salary outliers\n",
        "df = df[(df[\"Salary\"] > 30000) & (df[\"Salary\"] < 150000)]\n",
        "\n",
        "# Handle Years_Experience outliers (remove > 30 years)\n",
        "df = df[(df[\"Years_Experience\"]) <= 30]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Handling Outliers\n",
        "# -----------------------------\n",
        "#print(\"\\nDescriptive Statistics:\\n\", get.descriptive())  # <-- Fill: get descriptive stats\n",
        "print(\"\\nEmployees per Department:\\n\", df[\"Department\"].count())  # <-- Fill: count employees\n",
        "print(\"\\nRest Employees: \", df)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.boxplot(df[\"Salary\"])  # <-- Fill: column to visualize \"Salary\"\n",
        "plt.title(\"Salary Distribution After Handling Outliers\")\n",
        "plt.ylabel(df[\"Salary\"])   # <-- Fill: y-axis label (\"Salary\")\n",
        "plt.savefig(\"salary_boxplot.png\")  # Save the image\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ounpfxTaHXs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation and Splitting"
      ],
      "metadata": {
        "id": "K_SEoN9HHlM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "df = pd.read_csv(\"employees.csv\")   # <-- Fill: load employees.csv\n",
        "print(\"First 5 rows:\\n\", df.head())\n",
        "\n",
        "# Step 2: Features & Label\n",
        "X = df.drop(\"Promotion_Eligibility\", axis=1)  # Features\n",
        "y = df[\"Promotion_Eligibility\"]               # Label\n",
        "\n",
        "print(\"\\nFeatures:\\n\", X)       # <-- Fill\n",
        "print(\"\\nLabel:\\n\", y)          # <-- Fill\n",
        "\n",
        "# Step 3: Encoding Categorical Data\n",
        "# Label Encoding: Gender\n",
        "df[\"Gender_Label\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1})  # <-- Fill: Male/0, Female/1\n",
        "print(\"\\nAfter Label Encoding Gender:\\n\", df[[\"Name\",\"Gender\",\"Gender_Label\"]].head())\n",
        "\n",
        "# One-Hot Encoding: Department\n",
        "df_onehot = pd.get_dummies(df, columns=[\"Department\"])  # <-- Fill: column to one-hot encode\n",
        "print(\"\\nAfter One-Hot Encoding Department:\\n\", df_onehot.head())\n",
        "\n",
        "# Step 4: Feature Scaling\n",
        "# Normalization\n",
        "minmax = MinMaxScaler()\n",
        "X_normalized = minmax.fit_transform(df[[\"Age\", \"Salary\", \"Years_Experience\"]])         # <-- Fill: transform X numeric columns\n",
        "print(\"\\nAfter Normalization:\\n\", X_normalized[:5])\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(df[[\"Age\", \"Salary\", \"Years_Experience\"]])               # <-- Fill: transform X numeric columns\n",
        "print(\"\\nAfter Standardization:\\n\", X_standardized[:5])\n",
        "\n",
        "# Step 5: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size = 0.2, random_state = 42\n",
        ")   # <-- Fill: use train_test_split\n",
        "print(\"\\nTrain shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "LqizwP0_IBck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression - Practice Problem"
      ],
      "metadata": {
        "id": "4vdBqRnjIEkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv(\"employee_study.csv\")\n",
        "\n",
        "# Step 2: Select feature and label\n",
        "X = df[[\"Hours_Studied\"]]\n",
        "y = df[[\"Exam_Score\"]]\n",
        "\n",
        "# Step 3: Create the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# ✅ Step 5: Check prediction output for known input (training data)\n",
        "predicted_known = model.predict(X)\n",
        "print(\"Predictions for known inputs:\")\n",
        "print(predicted_known)\n",
        "\n",
        "# Step 6: Make prediction for a new input (6 hours)\n",
        "predicted_score = model.predict(pd.DataFrame([[6]], columns=[\"Hours_Studied\"]))\n",
        "print(\"Prediction for 6 hours studied:\")\n",
        "print(predicted_score)\n"
      ],
      "metadata": {
        "id": "js7KqHJqIoL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression - Practice Problem"
      ],
      "metadata": {
        "id": "ThYGPLQyI0Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Step 1: Input data\n",
        "speed = [[20], [40], [60], [80], [100]]     # Features\n",
        "mileage = [8, 15, 22, 20, 15]               # Labels\n",
        "\n",
        "# Step 2: Transform into polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "speed_poly = poly.fit_transform(speed)\n",
        "\n",
        "# Step 3: Create the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(speed_poly, mileage)\n",
        "\n",
        "# Step 5: Make prediction for 120 km/h\n",
        "predicted_mileage = model.predict(poly.transform([[120]]))\n",
        "print(\"Predicated mileage at 120 km/h: \")\n",
        "print(predicted_mileage)  # What do you think it will print?\n"
      ],
      "metadata": {
        "id": "1NDJX-CKJA1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error (MAE) - Practice Problem"
      ],
      "metadata": {
        "id": "UdQgQ9ttJJd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Actual vs Predicted values\n",
        "y_true = [30, 45, 50, 65, 80]\n",
        "y_pred = [28, 50, 47, 60, 85]\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_true , y_pred)\n",
        "print(\"MAE:\", mae)\n"
      ],
      "metadata": {
        "id": "CJRHpy6kJeDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error(MSE) - Practice Problem"
      ],
      "metadata": {
        "id": "FjwUhsa6JkkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Actual and Predicted Scores\n",
        "y_true = [72, 88, 95, 60]      # Actual values\n",
        "y_pred = [70, 90, 92, 65]      # Predicted values\n",
        "\n",
        "# Step 2: Calculate MSE\n",
        "mse = mean_squared_error(y_true, y_pred)  # <-- Calculate MSE here using sklearn\n",
        "rmse = math.sqrt(mse)  # <-- Calculate RMSE\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", round(mse, 2))\n",
        "print(\"Root Mean Squared Error (RMSE):\", round(rmse, 2))\n"
      ],
      "metadata": {
        "id": "qWjdc2lqJt21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R Square - Practice Problem"
      ],
      "metadata": {
        "id": "FG70a9vkJzsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Step 1: Create a dataset\n",
        "data = {\n",
        "    \"Sleep_Hours\": [4, 5, 6, 7, 8],\n",
        "    \"Productivity_Score\": [50, 60, 65, 75, 85]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features (X) and Labels (y)\n",
        "X = df[[\"Sleep_Hours\"]]   # Fill the feature column name\n",
        "y = df[\"Productivity_Score\"]     # Fill the label column name\n",
        "\n",
        "# Step 2: Train Linear Regression model\n",
        "model = LinearRegression()   # Fill the model class\n",
        "model.fit(X, y)   # Train the model\n",
        "\n",
        "# Step 3: Make predictions\n",
        "y_pred = model.predict(X)   # Make predictions\n",
        "\n",
        "# Step 4: Calculate R² score\n",
        "r2 = r2_score(y, y_pred)   # Fill the evaluation function\n",
        "\n",
        "print(\"Actual Scores:\", list(y))\n",
        "print(\"Predicted Scores:\", list(y_pred.round(2)))\n",
        "print(\"R² Score:\", round(r2, 2))\n"
      ],
      "metadata": {
        "id": "QyS5g8_NJ9_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting and Underfitting - Practice Problem"
      ],
      "metadata": {
        "id": "xDknnczGKA2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Step 0: New dataset\n",
        "X = np.array([2, 4, 6, 8, 10, 12, 14]).reshape(-1, 1)  # Hours studied\n",
        "y = np.array([30, 45, 55, 65, 70, 80, 85])             # Exam scores\n",
        "\n",
        "# Create test points for smooth curve plotting\n",
        "X_test = np.linspace(0, 15, 100).reshape(-1, 1)\n",
        "\n",
        "# Function to plot model predictions\n",
        "def plot_model(degree, title, subplot):\n",
        "    # Step 1: Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree)    #TODO Initialize polynomial features\n",
        "    X_poly = poly.fit_transform(X)              #TODO Transform X into polynomial features\n",
        "\n",
        "    # Step 2: Train Linear Regression model\n",
        "    model = LinearRegression()              #TODO Initialize the model\n",
        "    model.fit(X_poly, y)                        #TODO Train the model\n",
        "\n",
        "    # Step 3: Make predictions\n",
        "    X_test_poly = poly.transform(X_test)        #TODO Transform test points\n",
        "    y_pred = model.predict(X_test_poly)         #TODO Predict values\n",
        "\n",
        "    # Step 4: Plot\n",
        "    plt.subplot(1, 3, subplot)\n",
        "    plt.scatter(X, y, color=\"red\", label=\"Data Points\")\n",
        "    plt.plot(X_test, y_pred, color=\"blue\", label=f\"Degree {degree} Fit\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Study Hours\")\n",
        "    plt.ylabel(\"Exam Score\")\n",
        "    plt.legend()\n",
        "\n",
        "def main():\n",
        "    # Plot all three scenarios\n",
        "    plt.figure(figsize=(15, 4))\n",
        "\n",
        "    # Underfitting: Degree 1 (Linear)\n",
        "    plot_model(degree=1, title=\"Underfitting (Linear)\", subplot=1)\n",
        "\n",
        "    # Good Fit: Degree 2 (Quadratic)\n",
        "    plot_model(degree=2, title=\"Good Fit (Quadratic)\", subplot=2)\n",
        "\n",
        "    # Overfitting: Degree 6 (High Polynomial)\n",
        "    plot_model(degree=6, title=\"Overfitting (6th Degree)\", subplot=3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"fitting_examples.png\", dpi=300)  # Save as high-quality PNG\n",
        "    plt.close()  # Close the figure to free memory\n",
        "    print(\"Plots saved as fitting_examples.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "cj-HfOzzKJCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Project"
      ],
      "metadata": {
        "id": "0N1uULqUKugs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression Practice Problem with Plot and Main Function\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def linear_regression_task(csv_file=\"students.csv\", predict_hours=5, save_plot=\"regresion_plot.png\"):\n",
        "    \"\"\"\n",
        "    Performs Linear Regression on a dataset, predicts score, calculates MSE, and plots the regression line.\n",
        "    Returns:\n",
        "        predicted_score (float): Predicted score for given hours\n",
        "        mse (float): Mean Squared Error of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(\"students.csv\")  # Fill CSV filename\n",
        "\n",
        "    # Step 2: Define features (X) and target (y)\n",
        "    X = df[[\"Hours_Studied\"]]  # Fill feature column\n",
        "    y = df[\"Exam_Score\"]    # Fill target column\n",
        "\n",
        "    # Step 3: Create the linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Step 4: Fit the model\n",
        "    model.fit(X, y)  # Fill method to train the model\n",
        "\n",
        "    # Step 5: Make prediction\n",
        "    predicted_score = model.predict([[predict_hours]])[0]  # Fill method to predict\n",
        "    print(f\"Predicted exam score for studying {predict_hours} hours is: {predicted_score}\")\n",
        "\n",
        "    # Step 6: Calculate Mean Squared Error\n",
        "    y_pred = model.predict(X)  # Fill method to get predictions for MSE\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    print(f\"Mean Squared Error of the model: {mse}\")\n",
        "\n",
        "    # Step 7: Plot the data points and regression line\n",
        "    plt.scatter(X, y, color=\"blue\", label=\"Actual Data\")\n",
        "    plt.plot(X, y_pred, color=\"red\", label=\"Regression Line\")  # Fill predicted line\n",
        "    plt.xlabel(\"Hours Studied\")\n",
        "    plt.ylabel(\"Exam Score\")\n",
        "    plt.title(\"Linear Regression: Exam Score vs Hours Studied\")\n",
        "    plt.legend()\n",
        "    plt.savefig(save_plot)  # Save the plot as an image\n",
        "    plt.close()\n",
        "\n",
        "    return predicted_score, mse\n",
        "\n",
        "# Main function to run when script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    linear_regression_task()\n"
      ],
      "metadata": {
        "id": "drhc2tfwL2ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression Project"
      ],
      "metadata": {
        "id": "oFqmCGQsL_kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial Regression Practice Problem with Plot and Main Function\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def polynomial_regression_task(csv_file=\"students.csv\", degree=2, predict_hours=5, save_plot=\"regression_plot.png\"):\n",
        "    \"\"\"\n",
        "    Performs Polynomial Regression on a dataset, predicts score, calculates MSE, and plots the regression curve.\n",
        "\n",
        "    Returns:\n",
        "        predicted_score (float): Predicted score for given hours\n",
        "        mse (float): Mean Squared Error of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load the dataset\n",
        "    df = pd.read_csv(\"students.csv\")  # Fill CSV filename\n",
        "\n",
        "    # Step 2: Define features (X) and target (y)\n",
        "    X = df[[\"Hours_Studied\"]]  # Fill feature column\n",
        "    y = df[\"Exam_Score\"]    # Fill target column\n",
        "\n",
        "    # Step 3: Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    # Step 4: Create the linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Step 5: Fit the model\n",
        "    model.fit(X_poly, y)  # Fill method to train the model\n",
        "\n",
        "    # Step 6: Make prediction\n",
        "    predicted_score = model.predict(poly.transform([[predict_hours]]))[0]\n",
        "    print(f\"Predicted exam score for studying {predict_hours} hours is: {predicted_score}\")\n",
        "\n",
        "    # Step 7: Calculate Mean Squared Error\n",
        "    y_pred = model.predict(X_poly)  # Fill method to get predictions for MSE\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    print(f\"Mean Squared Error of the model: {mse}\")\n",
        "\n",
        "    # Step 8: Plot the data points and polynomial regression curve\n",
        "    plt.scatter(X, y, color=\"blue\", label=\"Actual Data\")\n",
        "\n",
        "    # Smooth curve for plotting\n",
        "    X_grid = np.linspace(min(X.values), max(X.values), 100).reshape(-1, 1)\n",
        "    plt.plot(X_grid, model.predict(poly.transform(X_grid)), color=\"red\", label=\"Polynomial Regression Curve\")\n",
        "\n",
        "    plt.xlabel(\"Hours Studied\")\n",
        "    plt.ylabel(\"Exam Score\")\n",
        "    plt.title(\"Polynomial Regression: Exam Score vs Hours Studied\")\n",
        "    plt.legend()\n",
        "    plt.savefig(save_plot)\n",
        "    plt.close()\n",
        "\n",
        "    return predicted_score, mse\n",
        "\n",
        "# Main function to run when script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    polynomial_regression_task()\n"
      ],
      "metadata": {
        "id": "LNwlZRNHMEZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression - Practice Problem"
      ],
      "metadata": {
        "id": "4PTj8AZWMHYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Dataset\n",
        "data = {\n",
        "    \"Sleep_Hours\": [3, 4, 5, 6, 7, 8],\n",
        "    \"Alert\": [0, 0, 0, 1, 1, 1]\n",
        "}\n",
        "\n",
        "# TODO: Create DataFrame from dictionary\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# TODO: Select features (X) and labels (y)\n",
        "X = df[[\"Sleep_Hours\"]]   # Feature\n",
        "y = df[\"Alert\"]     # Label\n",
        "\n",
        "# TODO: Create Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)   # Train the model\n",
        "\n",
        "# TODO: Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# TODO: Get probability estimates\n",
        "probabilities = model.predict_proba(X)\n",
        "print(\"Probabilities:\\n\", probabilities)\n"
      ],
      "metadata": {
        "id": "eQBv0PytMar9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Score to Probability - Practice Problem"
      ],
      "metadata": {
        "id": "ywG_RgaPMclV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    # TODO: Complete the formula for sigmoid\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Step 2: Example raw scores (like the linear step inside Logistic Regression)\n",
        "scores = np.array([-3.0, -0.5, 0.5, 2.5, 4.0])\n",
        "\n",
        "# Step 3: Convert scores to probabilities using sigmoid\n",
        "# TODO: Call the sigmoid function on scores\n",
        "probabilities = sigmoid(scores)\n",
        "\n",
        "# Step 4: Apply threshold (0.5) to get final predictions\n",
        "# TODO: Convert probabilities into 0 or 1 based on threshold\n",
        "predictions = (probabilities >= 0.5).astype(int)\n",
        "\n",
        "# Display results\n",
        "for s, p, pred in zip(scores, probabilities, predictions):\n",
        "    # TODO: Complete the print statement\n",
        "    print(f\"Score: {s:>4} -> Probability: {p:.2f} -> Prediction: {pred}\")\n"
      ],
      "metadata": {
        "id": "LT-t9l-NMnbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy with Code - Practice Problem"
      ],
      "metadata": {
        "id": "kUX1kXzfMpM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Define the actual labels (ground truth)\n",
        "# TODO: Fill in the actual labels\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Step 2: Define the predictions from the model\n",
        "# TODO: Fill in the predicted labels\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Step 3: Calculate accuracy\n",
        "# TODO: Call accuracy_score with y_true and y_pred\n",
        "acc = accuracy_score(y_true,y_pred)\n",
        "\n",
        "# Step 4: Print the accuracy\n",
        "# TODO: Complete the print statement\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "id": "mtVQD8kZM4T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and Recall - Practice Problem"
      ],
      "metadata": {
        "id": "JF3Ktcq1M6Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# TODO: Define the actual labels and predicted labels\n",
        "y_true = [0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0]\n",
        "y_pred = [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1]\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n"
      ],
      "metadata": {
        "id": "bk80uAtMNAh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1-Score - Practice Problem"
      ],
      "metadata": {
        "id": "ugUvHaM0NCHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Import the required function\n",
        "from sklearn.metrics import f1_score  # TODO\n",
        "\n",
        "# Actual labels (ground truth)\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 1, 0, 1]  # TODO: Fill the actual outcomes\n",
        "\n",
        "# Predictions from our model\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]  # TODO: Fill the predicted outcomes\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred)  # TODO: Use the correct function\n",
        "\n",
        "# Print result\n",
        "print(\"F1-score:\", f1)  # TODO: Ensure correct print format\n",
        "\n"
      ],
      "metadata": {
        "id": "Ss6EgM--NK30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project - Predict Students Result"
      ],
      "metadata": {
        "id": "0Lg3VH-SNsL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "def load_data(path):\n",
        "    # TODO: Read the CSV file using pandas\n",
        "    df = pd.read_csv(\"students.csv\")\n",
        "\n",
        "    # TODO: Select features (Hours_Studied should be a DataFrame, not Series)\n",
        "    X = df[['Hours_Studied']]\n",
        "\n",
        "    # TODO: Select target column (Exam_Pass)\n",
        "    y = df['Exam_Pass']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Step 2: Train logistic regression\n",
        "def train_model(X, y):\n",
        "    # TODO: Split the dataset into training and testing (30% test, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # TODO: Create logistic regression model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # TODO: Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model, X_test, y_test\n",
        "\n",
        "\n",
        "# Step 3: Evaluate predictions\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # TODO: Generate predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # TODO: Calculate Accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # TODO: Calculate Precision\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "\n",
        "    # TODO: Calculate Recall\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    # TODO: Calculate F1 Score\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "# Step 4: Display results\n",
        "def display_results(model, accuracy, precision, recall, f1):\n",
        "    print(\"Model used:\", type(model).__name__)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Provide the dataset path\n",
        "    path = \"students.csv\"\n",
        "\n",
        "    # TODO: Load dataset\n",
        "    X, y = load_data(path)\n",
        "\n",
        "    # TODO: Train logistic regression\n",
        "    model, X_test, y_test = train_model(X, y)\n",
        "\n",
        "    # TODO: Evaluate metrics\n",
        "    accuracy, precision, recall, f1 = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    # TODO: Print results\n",
        "    display_results(model, accuracy, precision, recall, f1)\n"
      ],
      "metadata": {
        "id": "3lH-1NzWNs5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree - Practice Problem"
      ],
      "metadata": {
        "id": "-Zsoa8ZGNvUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas for handling CSV/dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# Import DecisionTreeClassifier from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Step 1: Create the dataset\n",
        "data = {\n",
        "    \"Study_Hours\": [2, 3, 4, 5, 6],    # Hours studied\n",
        "    \"Attendance\": [60, 70, 80, 85, 90],  # Attendance percentages\n",
        "    \"Pass\": [0, 0, 1, 1, 1]             # Target labels (0 = Fail, 1 = Pass)\n",
        "}\n",
        "\n",
        "# TODO: Convert the dictionary to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Separate features and label\n",
        "# TODO: Select Study_Hours and Attendance as features (X)\n",
        "X = df[[\"Study_Hours\", \"Attendance\"]]\n",
        "\n",
        "# TODO: Select Pass as the target label (y)\n",
        "y = df[\"Pass\"]\n",
        "\n",
        "# Step 3: Train the Decision Tree model\n",
        "# TODO: Create a DecisionTreeClassifier with random_state=42\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# TODO: Fit the model on the training data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 4: Make predictions on the training data\n",
        "# TODO: Generate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# TODO: Print the predictions as a list\n",
        "print(\"Predictions:\", predictions.tolist())\n"
      ],
      "metadata": {
        "id": "l76caU9EN6_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest - Worked Example"
      ],
      "metadata": {
        "id": "bDMkzMYwN8qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"Study_Hours\": [2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"Attendance\": [60, 65, 70, 75, 80, 62, 68, 72, 78, 85, 88, 90, 95],\n",
        "    \"Pass\":        [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[[\"Study_Hours\", \"Attendance\"]]\n",
        "y = df[\"Pass\"]\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Predict probabilities\n",
        "probabilities = model.predict_proba(X)\n",
        "\n",
        "# Round to 1 decimal\n",
        "probabilities = np.round(probabilities, 1)\n",
        "\n",
        "#Print the Predicted Probabilities\n",
        "print(\"Predictions:\", predictions.tolist())\n",
        "\n",
        "# Print with comments\n",
        "for row in probabilities:\n",
        "    fail, pass_ = row\n",
        "    print(f\"[{fail:.1f} {pass_:.1f}]   # {int(fail*100)}% Fail, {int(pass_*100)}% Pass\")\n"
      ],
      "metadata": {
        "id": "hQpivGPSOBzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes - Practice Problem"
      ],
      "metadata": {
        "id": "pUzX5vkvODjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Import pandas for handling CSV\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Import CountVectorizer and MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Step 1: Load dataset\n",
        "# TODO: Load your CSV file \"student_notifications.csv\"\n",
        "df = pd.read_csv(\"student_notifications.csv\")\n",
        "\n",
        "# Step 2: Convert text into numeric features\n",
        "# TODO: Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# TODO: Transform the messages into features\n",
        "X = vectorizer.fit_transform(df[\"Message\"])\n",
        "\n",
        "# Step 3: Extract target labels\n",
        "# TODO: Select the target column\n",
        "y = df[\"Spam\"]\n",
        "\n",
        "# TODO: Print feature names\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out(), \"\\n\")\n",
        "\n",
        "# Step 4: Train the Naive Bayes model\n",
        "# TODO: Initialize MultinomialNB\n",
        "model = MultinomialNB()\n",
        "\n",
        "# TODO: Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "# TODO: Predict on the same dataset\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions.tolist())\n",
        "\n",
        "# Step 6: Get probability estimates\n",
        "# TODO: Predict probabilities for each message\n",
        "probabilities = model.predict_proba(X)\n",
        "\n",
        "# Step 7: Print probabilities with comments\n",
        "for msg, row in zip(df[\"Message\"], probabilities):\n",
        "    not_spam, spam = row\n",
        "    print(f\"Message: {msg}\\n[{not_spam:.2f} {spam:.2f}]   # Probabilities\\n\")\n"
      ],
      "metadata": {
        "id": "VRW9SztlOLsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project – Creditcard Fraud Detection"
      ],
      "metadata": {
        "id": "wT2wKpQVOsC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def train_credit_card_fraud_model():\n",
        "    \"\"\"\n",
        "    Loads the dataset, scales features, splits into train-test sets,\n",
        "    trains MultinomialNB, Decision Tree, and Random Forest,\n",
        "    and returns key variables for testing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load Dataset\n",
        "    df = pd.read_csv(\"creditcard.csv\")          # TODO Completed\n",
        "\n",
        "    # Step 2: Prepare Features and Target\n",
        "    X = df.drop(\"Class\", axis=1)                # TODO: all columns except target\n",
        "    y = df[\"Class\"]                             # TODO: target column (0 Not Fraud, 1 Fraud)\n",
        "\n",
        "    # Step 3: Scale Features to [0, 1]\n",
        "    scaler = MinMaxScaler()                     # TODO: Initialize MinMaxScaler\n",
        "    X_scaled = scaler.fit_transform(X)          # TODO: Fit and transform X\n",
        "\n",
        "    # Step 4: Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled,\n",
        "        y,\n",
        "        test_size=0.2,                          # TODO: test_size\n",
        "        random_state=42,\n",
        "        stratify=y                              # Maintain fraud ratio\n",
        "    )\n",
        "\n",
        "    # Step 5: Define Models\n",
        "    models = {\n",
        "        \"MultinomialNB\": MultinomialNB(),               # TODO\n",
        "        \"DecisionTree\": DecisionTreeClassifier(),       # TODO\n",
        "        \"RandomForest\": RandomForestClassifier()        # TODO\n",
        "    }\n",
        "\n",
        "    # Step 6: Train and Evaluate\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)                     # TODO: Train the model\n",
        "        preds = model.predict(X_test)                   # TODO: Make predictions\n",
        "        acc = accuracy_score(y_test, preds)             # TODO: Calculate accuracy\n",
        "\n",
        "        print(f\"{name} Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "    # Return important variables for testing\n",
        "    return models, scaler, X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Run training only when executing script directly\n",
        "if _name_ == \"_main_\":\n",
        "    train_credit_card_fraud_model()\n"
      ],
      "metadata": {
        "id": "PS6ZpBG6PF8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project – Clean IMBD Movie Dataset"
      ],
      "metadata": {
        "id": "ahy2ztKEPgsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Initial Setup: NLTK Stopwords\n",
        "# ---------------------------------------------------------\n",
        "try:\n",
        "    STOP_WORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords')\n",
        "    STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Function: clean_text\n",
        "# ---------------------------------------------------------\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans a raw string of text by performing:\n",
        "      1. Remove HTML tags\n",
        "      2. Convert to lowercase\n",
        "      3. Keep only letters (a-z) and spaces\n",
        "      4. Tokenize into words\n",
        "      5. Remove stopwords\n",
        "      6. Join cleaned words back into a string\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "    # 2. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Keep only alphabets and spaces\n",
        "    text = re.sub(r'[^a-z ]', ' ', text)\n",
        "\n",
        "    # 4. Tokenize\n",
        "    words = text.split()\n",
        "\n",
        "    # 5. Remove stopwords\n",
        "    words = [word for word in words if word not in STOP_WORDS]\n",
        "\n",
        "    # 6. Join back\n",
        "    cleaned_text = \" \".join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Main Execution Pipeline\n",
        "# ---------------------------------------------------------\n",
        "def main():\n",
        "\n",
        "    INPUT_FILE = 'IMDB Dataset.csv'\n",
        "    OUTPUT_FILE = 'cleaned_imdb_dataset.csv'\n",
        "\n",
        "    print(f\"Loading data from '{INPUT_FILE}'...\")\n",
        "\n",
        "    # 1. Load dataset\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset not found at '{INPUT_FILE}'.\")\n",
        "        print(\"Please download it and place it in the correct directory.\")\n",
        "        return\n",
        "\n",
        "    # 2. Apply cleaning function\n",
        "    print(\"Applying text cleaning function to all reviews...\")\n",
        "    df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "    # 3. Keep only cleaned_review + sentiment\n",
        "    final_df = df[['cleaned_review', 'sentiment']]\n",
        "\n",
        "    # 4. Save output\n",
        "    print(f\"Saving cleaned data to '{OUTPUT_FILE}'...\")\n",
        "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(\"Data cleaning process complete.\")\n",
        "    print(f\"Cleaned data saved to '{OUTPUT_FILE}'\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Script Entry Point\n",
        "# ---------------------------------------------------------\n",
        "if _name_ == \"_main_\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aFfS_qDdPjVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and Saving the Sentiment Model"
      ],
      "metadata": {
        "id": "UPEG4IbQPnyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# File Paths\n",
        "INPUT_FILE = 'cleaned_imdb_dataset.csv'\n",
        "OUTPUT_FILE = 'sentiment_model.pkl'\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# MODEL TRAINING FUNCTION\n",
        "# -------------------------------------------------------------\n",
        "def train_model():\n",
        "    \"\"\"Main function to train and save the sentiment analysis model.\"\"\"\n",
        "\n",
        "    print(\"--- Starting Model Training ---\")\n",
        "\n",
        "    try:\n",
        "        # Task 1: Load the Data\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "        # Task 2: Prepare Features and Target\n",
        "        X = df['cleaned_review']\n",
        "        y = df['sentiment']\n",
        "\n",
        "        # Task 3: Train-Test Split (80/20)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Task 4: Vectorize the Text\n",
        "        vectorizer = CountVectorizer()\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "        X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "        # Task 5: Train the Model\n",
        "        model = MultinomialNB()\n",
        "        model.fit(X_train_vec, y_train)\n",
        "\n",
        "        # Task 6: Evaluate Performance\n",
        "        predictions = model.predict(X_test_vec)\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "        print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        # Task 7: Save Vectorizer + Model as Artifacts\n",
        "        artifacts = {\n",
        "            'vectorizer': vectorizer,\n",
        "            'model': model\n",
        "        }\n",
        "\n",
        "        with open(OUTPUT_FILE, 'wb') as f:\n",
        "            pickle.dump(artifacts, f)\n",
        "\n",
        "        print(f\"--- Model and Vectorizer saved successfully to '{OUTPUT_FILE}' ---\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The input file '{INPUT_FILE}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# TEXT CLEANING FOR PREDICTION\n",
        "# -------------------------------------------------------------\n",
        "def clean_text(text):\n",
        "    \"\"\"A simple function to clean text before prediction.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation & numbers\n",
        "    return text\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# PREDICTION EXAMPLE\n",
        "# -------------------------------------------------------------\n",
        "def run_prediction_example():\n",
        "    \"\"\"Loads the saved model and runs predictions on sample reviews.\"\"\"\n",
        "\n",
        "    print(\"\\n--- Running Prediction Example ---\")\n",
        "\n",
        "    try:\n",
        "        with open(OUTPUT_FILE, 'rb') as f:\n",
        "            artifacts = pickle.load(f)\n",
        "\n",
        "        vectorizer = artifacts['vectorizer']\n",
        "        model = artifacts['model']\n",
        "\n",
        "        sample_reviews = [\n",
        "            \"This movie was absolutely fantastic, one of the best I have seen all year!\",\n",
        "            \"A complete waste of time. The plot was boring and the acting was terrible.\",\n",
        "            \"It was an okay film, not great but not bad either.\"\n",
        "        ]\n",
        "\n",
        "        for review in sample_reviews:\n",
        "            cleaned_review = clean_text(review)\n",
        "            review_vec = vectorizer.transform([cleaned_review])\n",
        "            prediction = model.predict(review_vec)[0]\n",
        "\n",
        "            print(f\"\\nReview: '{review}'\")\n",
        "            print(f\"Predicted Sentiment: {prediction}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Could not load '{OUTPUT_FILE}'. Please train the model first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# SCRIPT ENTRY POINT\n",
        "# -------------------------------------------------------------\n",
        "if _name_ == \"_main_\":\n",
        "    train_model()\n",
        "    run_prediction_example()\n"
      ],
      "metadata": {
        "id": "QJVJ9gM3PqcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 - Titanic Survival Prediction"
      ],
      "metadata": {
        "id": "Hk5AKVbjPwzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load training data\n",
        "data = pd.read_csv(\"Titanic.csv\")\n",
        "\n",
        "# Keep only weaker baseline features\n",
        "features = [\"Pclass\", \"Sex\"]\n",
        "target = \"Survived\"\n",
        "data = data[features + [target]]\n",
        "\n",
        "# Convert categorical to numeric\n",
        "data[\"Sex\"] = data[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Split into features and target\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Optional: Train-Test Split for fair evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression (low C → stronger regularization → lower accuracy)\n",
        "model = LogisticRegression(max_iter=500, C=0.1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "print(f\"Baseline Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "XD75WxWsQKSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 - Build a Better Titanic Prediction Model"
      ],
      "metadata": {
        "id": "M1Rc-80hQOB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# from sklearn.ensemble import RandomForestClassifier  # Optional upgrade\n",
        "\n",
        "def preprocess(df):\n",
        "    # Nothing to drop, only encode categorical if needed\n",
        "    # Here 'Sex' is already numeric (0/1), so nothing to encode\n",
        "    # Fill missing numeric values with median\n",
        "    df = df.fillna(df.median(numeric_only=True))\n",
        "    return df\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(\"Titanic_train.csv\")\n",
        "\n",
        "    # Separate features and target\n",
        "    y_train = train_data[\"Survived\"]\n",
        "    X_train = preprocess(train_data.drop(columns=[\"Survived\"]))\n",
        "\n",
        "    # Train Decision Tree\n",
        "    model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(\"Titanic_test.csv\")\n",
        "    X_test = preprocess(test_data)\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    return preds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    preds = train_and_evaluate()\n",
        "\n",
        "    # This will calculate the accuracy of your code\n",
        "    true_labels = pd.read_csv(\"Titanic_test_labels.csv\")[\"Survived\"]\n",
        "    acc = accuracy_score(true_labels, preds)\n",
        "\n",
        "    print(f\"The model accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "rQOWeT9tQSAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means on a Toy Dataset - Practice Problem"
      ],
      "metadata": {
        "id": "TAh2W0HcQehs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate a toy dataset\n",
        "# TODO: Create a dataset with 150 samples and 4 clusters\n",
        "X, _ = make_blobs(n_samples=100, centers=4, random_state=42)\n",
        "\n",
        "# Step 2: Train KMeans\n",
        "# TODO: Initialize KMeans with 4 clusters\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "\n",
        "# TODO: Fit the model and get cluster labels\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 3: Visualize the clusters\n",
        "# TODO: Plot the points with cluster colors\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "\n",
        "# TODO: Plot the cluster centers (centroids)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', marker='X', s=200, label=\"Centroids\")\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig(\"kmeans_practice.png\")\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "Us6rbHGwQpRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "azwlulbVQuTs"
      }
    }
  ]
}